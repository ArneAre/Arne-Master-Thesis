{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1141d78-39aa-42bb-a3c9-bba0d81b0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frex_by_topic = pd.read_csv(\"C:\\\\Users\\\\arnea\\\\OneDrive\\\\Desktop\\\\Thesis\\\\Work\\\\Python\\\\top_fre_words_per_topic.csv\")\n",
    "\n",
    "frex_by_topic = frex_by_topic.rename(columns={'Word': 'word_stemmed'})\n",
    "frex_by_topic = frex_by_topic.rename(columns={'Topic': 'topic'})\n",
    "frex_by_topic = frex_by_topic.rename(columns={'Probability': 'frex'})\n",
    "\n",
    "total_words = frex_by_topic.shape[0]\n",
    "print(\"\\nNumber of words:\", total_words)\n",
    "\n",
    "unique_topics_count = frex_by_topic['topic'].nunique()\n",
    "print(\"Number of unique topics:\", unique_topics_count)\n",
    "print(\"Words per topic:\", 5100 / 17)\n",
    "\n",
    "frex_by_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd218bb-c741-4955-894e-3d0817468b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and merge to have the stemmed version of the words\n",
    "dictionary_stem = pd.read_csv(\"CC:\\\\Users\\\\arnea\\\\OneDrive\\\\Desktop\\\\Thesis\\\\Work\\\\Python\\\\dictionary_stem.csv\") \n",
    "\n",
    "# Calculate the total number of different articles (rows) as total\n",
    "total_words = dictionary_stem.shape[0]\n",
    "print(\"\\nNumber of words:\", total_words)\n",
    "\n",
    "dictionary_stem.head()\n",
    "\n",
    "# merge the two dataframes\n",
    "df = pd.merge(dictionary_stem, frex_by_topic, on='word_stemmed', how='right')\n",
    "\n",
    "total_words = df.shape[0]\n",
    "print(\"\\nNumber of words:\", total_words)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9922a51-b5f9-4d6d-8e8e-c40f174fc215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below is not very good. Use this tutorial instead, maybe with an external RA.\n",
    "#https://diging.github.io/tethne/doc/0.6.1-beta/tutorial.mallet.html\n",
    "#To calculate the 5 strongest connections between topics based on the shared first 30 words, we need to sort the edges by their weight (number of shared words) and select the top 5 connections.\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'frex_by_topic' DataFrame has 'topic' and 'word_stemmed' columns\n",
    "\n",
    "# Group the words by topic into a dictionary\n",
    "groups = frex_by_topic.groupby('topic')['word_stemmed'].apply(list).to_dict()\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes (topics) to the graph\n",
    "G.add_nodes_from(groups.keys())\n",
    "\n",
    "# Add edges between topics that share common words (only considering the first 300 words for each topic)\n",
    "for group1, words1 in groups.items():\n",
    "    for group2, words2 in groups.items():\n",
    "        if group1 != group2:\n",
    "            shared_words = set(words1[:300]).intersection(words2[:300])  # Consider only the first 30 words\n",
    "            if shared_words:\n",
    "                G.add_edge(group1, group2, weight=len(shared_words))\n",
    "\n",
    "# Sort edges by weight (number of shared words) in descending order\n",
    "sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "# Assign colors to nodes based on topics\n",
    "# You can define your color mapping for different topics here\n",
    "# For simplicity, let's generate random colors for each topic\n",
    "import random\n",
    "group_colors = {topic: \"#\" + ''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for topic in G.nodes()}\n",
    "\n",
    "# Get node colors based on topics\n",
    "node_colors = [group_colors[topic] for topic in G.nodes()]\n",
    "\n",
    "# Draw the graph with reduced font size for labels\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, seed=42)  # You can use different layouts\n",
    "nx.draw(G, pos, with_labels=True, node_color=node_colors, node_size=500, font_weight='bold', font_size=8)  # Adjust font_size here\n",
    "\n",
    "# Highlight the top 5 strongest connections\n",
    "for edge in strongest_connections:\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=[(edge[0], edge[1])], width=2, edge_color='red')\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Topic Correlation based on shared FREX words (first 30 words). In red the 4 strongest connections.')\n",
    "plt.show()\n",
    "# Sort edges by weight (number of shared words) in descending order\n",
    "sorted_edges = sorted(G.edges(data=True), key=lambda x: x[2]['weight'], reverse=True)\n",
    "\n",
    "# Get the top 5 strongest connections\n",
    "strongest_connections = sorted_edges[:10]\n",
    "\n",
    "# Create a list to store the strongest connection pairs and their strength\n",
    "strongest_connection_list = []\n",
    "\n",
    "# Print the top 10 strongest connections and store them in the list\n",
    "print(\"Top 10 Strongest Connections Pairs:\")\n",
    "for edge in strongest_connections:\n",
    "    strength = edge[2]['weight']\n",
    "    connection_pair = (edge[0], edge[1], strength)\n",
    "    strongest_connection_list.append(connection_pair)\n",
    "    print(edge[:2], \"Strength:\", strength)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792310b5-73e1-4d61-8ebe-3e5278e08c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "# Initializing VADER SentimentIntensityAnalyzer\n",
    "sia = SIA()\n",
    "\n",
    "# Calculating sentiment for each word in the 'term' column\n",
    "results_word_sentiment = []\n",
    "for idx, row in df.iterrows():\n",
    "    word = str(row['word'])  # Convert 'word' to string explicitly\n",
    "    pol_score = sia.polarity_scores(word)\n",
    "    pol_score['word'] = word\n",
    "    pol_score['word_stemmed'] = row['word_stemmed']\n",
    "    pol_score['frex'] = row['frex']\n",
    "    pol_score['topic'] = row['topic']\n",
    "    results_word_sentiment.append(pol_score)\n",
    "    \n",
    "# Creating a DataFrame from the sentiment scores of each word\n",
    "word_sentiment_scores = pd.DataFrame(results_word_sentiment)\n",
    "\n",
    "# Displaying the sentiment scores for each word\n",
    "print(word_sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c3cd1-e5c3-4e3a-b18d-d60154e641e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plot for compound sentiment scores\n",
    "sns.kdeplot(data=word_sentiment_scores['compound'], fill=True, color='blue', label='Compound')\n",
    "# KDE plot for neg sentiment scores\n",
    "sns.kdeplot(data=word_sentiment_scores['neg'], fill=True, color='orange', label='Neg')\n",
    "\n",
    "\n",
    "plt.title('Kernel Density Estimate of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Scores')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()  # Show legend with labels for compound and neg\n",
    "plt.show()\n",
    "\n",
    "# Multiply 'frex' with 'neg', 'neu', 'pos', 'compound' columns\n",
    "word_sentiment_scores['neg_weighted'] = word_sentiment_scores['frex'] * word_sentiment_scores['neg']\n",
    "word_sentiment_scores['neu_weighted'] = word_sentiment_scores['frex'] * word_sentiment_scores['neu']\n",
    "word_sentiment_scores['pos_weighted'] = word_sentiment_scores['frex'] * word_sentiment_scores['pos']\n",
    "word_sentiment_scores['compound_weighted'] = word_sentiment_scores['frex'] * word_sentiment_scores['compound']\n",
    "\n",
    "# Replace 0 values with 0 in the new weighted columns\n",
    "columns_to_adjust = ['neg_weighted', 'neu_weighted', 'pos_weighted', 'compound_weighted']\n",
    "word_sentiment_scores[columns_to_adjust] = word_sentiment_scores[columns_to_adjust].where(word_sentiment_scores[columns_to_adjust] != 0, 0)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(word_sentiment_scores.head())\n",
    "\n",
    "# Selecting specific columns\n",
    "selected_columns = ['word', 'neg', 'neu', 'pos']\n",
    "selected_data = word_sentiment_scores[selected_columns]\n",
    "\n",
    "# Export the selected columns to an Excel file\n",
    "#selected_data.to_excel('frex_word_sentiment_scores.xlsx', index=False)\n",
    "\n",
    "# Displaying a confirmation message\n",
    "print(\"Export to 'frex_word_sentiment_scores.xlsx' completed successfully.\")\n",
    "\n",
    "# Grouping by 'topic' and topic_labels, aggregating the weighted columns by sum\n",
    "aggregate_sentiment_by_topic = word_sentiment_scores.groupby('topic')[['neg_weighted', 'neu_weighted', 'pos_weighted', 'compound_weighted']].sum()\n",
    "\n",
    "# Displaying the aggregate sentiment scores by 'topic'\n",
    "print(aggregate_sentiment_by_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d0aa0-c6d9-4866-b992-138e36ca2e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the 3 different variables \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Columns to normalize\n",
    "columns_to_normalize = ['compound_weighted', 'neg_weighted', 'pos_weighted']\n",
    "\n",
    "# Fit and transform the data to normalize it between -1 and 1 for each column\n",
    "normalized_values = scaler.fit_transform(aggregate_sentiment_by_topic[columns_to_normalize])\n",
    "\n",
    "# Create a DataFrame with the normalized values\n",
    "normalized_df = pd.DataFrame(normalized_values, columns=columns_to_normalize, index=aggregate_sentiment_by_topic.index)\n",
    "\n",
    "# Update the original DataFrame with normalized values\n",
    "aggregate_sentiment_by_topic[columns_to_normalize] = normalized_df\n",
    "\n",
    "print(aggregate_sentiment_by_topic)\n",
    "\n",
    "# This is the original plot, however I manually changed the colors such that values <0.2 are yellow now\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "# Sort the 'topic_labels' based on 'compound_weighted' values in descending order\n",
    "sorted_topic_labels = aggregate_sentiment_by_topic.sort_values(by='compound_weighted', ascending=True).index\n",
    "\n",
    "# Reorder 'compound_weighted' values based on the sorted labels\n",
    "sorted_compound_weighted_values = aggregate_sentiment_by_topic.loc[sorted_topic_labels, 'compound_weighted']\n",
    "\n",
    "# Define colormap and normalize your data to map it to colors\n",
    "cmap = plt.get_cmap('RdYlGn')\n",
    "norm = Normalize(vmin=min(sorted_compound_weighted_values), vmax=max(sorted_compound_weighted_values))\n",
    "\n",
    "# Calculate colors based on normalized values\n",
    "colors = [cmap(norm(value)) for value in sorted_compound_weighted_values]\n",
    "\n",
    "# Create a horizontal bar chart with colored bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.barh(sorted_topic_labels, sorted_compound_weighted_values, color=colors)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title(\"Topic sentiment using the FREX-approach (300 words/topic)\")\n",
    "plt.xlabel(\"Aggregate weighted compound sentiment, normalized and sorted in descending order\")\n",
    "plt.ylabel(\"Topic\")\n",
    "\n",
    "# Create a colorbar to indicate values\n",
    "sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "sm.set_array([])  \n",
    "cbar = plt.colorbar(sm, ax=plt.gca())  # Explicitly specify the axes for colorbar\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d6b55-6493-4a3e-ba60-ea829cc9c5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a colormap with 15 distinct colors\n",
    "colors = plt.cm.tab20.colors  # You can use other colormaps as per your preference\n",
    "\n",
    "# Create an array of marker styles for distinguishing between different topics\n",
    "markers = ['o', 's', 'D', '^', 'v', '<', '>', 'p', 'P', '*', 'h', 'H', '+', 'x', '|']\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterate over each topic label (which is assumed to be the index)\n",
    "for i, (index, row) in enumerate(aggregate_sentiment_by_topic.iterrows()):\n",
    "    topic_label = index\n",
    "    plt.scatter(row['pos_weighted'], row['neg_weighted'], label=topic_label, s=100,\n",
    "                color=colors[i], marker=markers[i % len(markers)])\n",
    "\n",
    "# Set plot limits and labels\n",
    "plt.xlim(-1.1, 1.1)\n",
    "plt.ylim(-1.1, 1.1)\n",
    "plt.xlabel('Weighted positive sentiment')\n",
    "plt.ylabel('Weighted negative sentiment')\n",
    "\n",
    "# Show a legend with the topic labels\n",
    "plt.legend(title='Topic', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Set plot title\n",
    "plt.title('Scatterplot for negative and positive sentiments by topic')\n",
    "\n",
    "# Show the plot\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dtm2)",
   "language": "python",
   "name": "dtm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
